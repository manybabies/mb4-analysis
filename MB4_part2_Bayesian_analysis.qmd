---
title: "MB4 Main Analyses: Data Import"
author: "Kelsey Lucca, Arthur Capelier-Mourguy, Mike Frank, Yiyi Wang, Alvin W.M. Tan, & Francis Yuen" (needs to update)
date: "2024-04-08"
output: word_document
---


```{r options, message=F}
knitr::opts_chunk$set(cache = TRUE, message = F)
```

```{r packages}
### descriptives
library(irr)
library(ICCbin)

### modeling
library(lme4)
library(brms)
library(binom)
library(MCMCpack)
library(bridgesampling)
library(coda)
library(ggeffects)
library(meta)

### utils
library(zoo)
library(here)
library(knitr)
library(future)
# plan(multiprocess, workers = 4)
library(tidyverse) 
library(assertthat)

# custom functions 
source("StatTools.R") 
source("geom_flat_violin.R")

```

```{r setup}
source("geom_flat_violin.R")

theme_set(theme_bw(base_size = 10))
set.seed(705)
```

```{r}
participant_summaries <- readRDS(here("intermediates", "participant_summaries.rds"))
primary_data <- readRDS(here("intermediates", "primary_data.rds"))
```




# Data analysis

We first produce some diagnostic plots. We then define the full Bayesian model against which null models will be compared. Finally we address the two research questions: do infants in the social condition choose preferably the helper character, and does preference for either character change with age.

## Diagnostic plots

We can first check how infants in the two conditions performed in general depending on their age.
```{r proportion}
# Calculate mean proportions for each condition
proportions <- primary_data %>%
  group_by(condition) %>%
  summarize(
    mean_proportion = mean(chose_helper)
  )

# Print the results
print(paste0("Proportion of helper choice in the social condition: ", proportions$mean_proportion[proportions$condition == "social"]))
print(paste0("Proportion of helper choice in the nonsocial condition: ", proportions$mean_proportion[proportions$condition == "nonsocial"]))

```



```{r age_plot, message=FALSE}
scatter <- ggplot(primary_data,
                  aes(x = z_age_days,
                      y = chose_helper,
                      colour = condition)) +
  stat_smooth(method = "lm") +
  geom_point(position = position_jitter(height = .05, width = 0),
             size = 1) +
  xlab("Age (scaled centred)") + ylab("Choice") + theme(legend.position = "top") +
  scale_colour_brewer(palette = "Dark2", name = "Condition",
                      breaks = c("nonsocial", "social"),
                      labels = c("non-social", "social")) +
  scale_y_continuous(breaks = c(0, 1),
                     labels = c("Hinderer/\nPush-down character", "Helper/\nPush-up character"))
ggsave("age_scatter.pdf", scatter,
       units = "mm", width = 180, height = 100, dpi = 1000)
(scatter)
```
We also check how infants in the two conditions performed in general depending on the number of habituation trials.

```{r number of habituation trial plot, message=FALSE}
scatter <- ggplot(primary_data,
                  aes(x = habituation_trial,
                      y = chose_helper,
                      colour = condition)) +
  stat_smooth(method = "lm") +
  geom_point(position = position_jitter(height = .05, width = 0),
             size = 1) +
  xlab("number of habituation trials") + ylab("Choice") + theme(legend.position = "top") +
  scale_colour_brewer(palette = "Dark2", name = "Condition",
                      breaks = c("nonsocial", "social"),
                      labels = c("non-social", "social")) +
  scale_y_continuous(breaks = c(0, 1),
                     labels = c("Hinderer/\nPush-down character", "Helper/\nPush-up character"))
ggsave("habituation_trial_scatter.pdf", scatter,
      units = "mm", width = 180, height = 100, dpi = 1000)
(scatter)
```

We also check how infants in the two conditions performed in general depending on looking time during habituation.

```{r total looking time plot, message=FALSE}
primary_data$num_should <- as.numeric(primary_data$total_looking)
scatter <- ggplot(primary_data,
                  aes(x = total_looking,
                      y = chose_helper,
                      colour = condition)) +
  stat_smooth(method = "lm") +
  geom_point(position = position_jitter(height = .05, width = 0),
             size = 1) +
  xlab("looking time") + ylab("Choice") + theme(legend.position = "top") +
  scale_colour_brewer(palette = "Dark2", name = "Condition",
                      breaks = c("nonsocial", "social"),
                      labels = c("non-social", "social")) +
  scale_y_continuous(breaks = c(0, 1),
                     labels = c("Hinderer/\nPush-down character", "Helper/\nPush-up character"))
ggsave("looking_scatter.pdf", scatter,
       units = "mm", width = 180, height = 100, dpi = 1000)
(scatter)
```


```{r habituation plot, message=FALSE}
primary_data$habituation <- ifelse(is.na(primary_data$habituation_trial),0,1)
scatter <- ggplot(primary_data,
                  aes(x = habituation,
                      y = chose_helper,
                      colour = condition)) +
  stat_smooth(method = "lm") +
  geom_point(position = position_jitter(height = .05, width = 0),
             size = 1) +
  xlab("habituation") + ylab("Choice") + theme(legend.position = "top") +
  scale_colour_brewer(palette = "Dark2", name = "Condition",
                      breaks = c("nonsocial", "social"),
                      labels = c("non-social", "social")) +
  scale_y_continuous(breaks = c(0, 1),
                     labels = c("Hinderer/\nPush-down character", "Helper/\nPush-up character"))+
    scale_x_continuous(breaks = c(0, 1),
                     labels = c("Not habituated", "Habituated"))
ggsave("habituation_scatter.pdf", scatter,
       units = "mm", width = 180, height = 100, dpi = 1000)
(scatter)
```

```{r screen, message=FALSE}
scatter <- ggplot(primary_data,
                  aes(x = screen_size_inches,
                      y = chose_helper,
                      colour = condition)) +
  stat_smooth(method = "lm") +
  geom_point(position = position_jitter(height = .05, width = 0),
             size = 1) +
  xlab("screen size (inches)") + ylab("Choice") + theme(legend.position = "top") +
  scale_colour_brewer(palette = "Dark2", name = "Condition",
                      breaks = c("nonsocial", "social"),
                      labels = c("non-social", "social")) +
  scale_y_continuous(breaks = c(0, 1),
                     labels = c("Hinderer/\nPush-down character", "Helper/\nPush-up character"))
ggsave("screen_size_inches_scatter.pdf", scatter,
       units = "mm", width = 180, height = 100, dpi = 1000)
(scatter)
```

We can then check lab variability, by plotting the estimated mean and Credible Intervals per lab for each condition.

```{r lab_plot, message=FALSE}
by_lab <- primary_data %>%
  group_by(lab_id, condition) %>%
  summarize(tested = n(),
            chose_helper_mean = mean(chose_helper), 
            chose_helper = sum(chose_helper),
            ci_lower = binom.bayes(x = chose_helper, 
                                   n = tested)$lower,
            ci_upper = binom.bayes(x = chose_helper, 
                                   n = tested)$upper) %>%
  mutate(condition = factor(condition, c("nonsocial", "social")))

forest <- ggplot(by_lab,
                 aes(x = lab_id, colour = condition,
                     y = chose_helper_mean, ymin = ci_lower, ymax = ci_upper)) + 
  geom_hline(yintercept = .5, col = "black", lty = 2) + 
  geom_linerange(position = position_dodge(width = .5)) + 
  geom_point(aes(size = tested), position = position_dodge(width = .5)) + 
  coord_flip() + xlab("Lab") + ylab("Proportion Choosing Helper/Push-up character") + ylim(0,1) + 
  scale_colour_brewer(palette = "Dark2", name = "Condition",
                      breaks = c("nonsocial", "social"),
                      labels = c("non-social", "social")) +
  scale_size_continuous(name = "N", breaks = function(x) c(min(x), mean(x), max(x))) + 
  theme(legend.position = "bottom")
ggsave("forest.pdf", forest,
       units = "mm", width = 180, height = 100, dpi = 1000)
(forest)
```

## Bayesian analysis

### Global Bayesian model

We first need to define the full model that will be used throughout the Bayesian analysis, and define appropriate priors for this model. We define both a model with informative priors based on the meta-analysis by Margoni and Surian (2018), and a model with non-informative priors to check for the sensitivity of our results to the choice of priors. For the non-informative priors, we only specify a narrower prior for the random effects than the default one implemented in `brms` in order to improve model fit.

```{r full_model, results=FALSE, message=FALSE}
# Define priors
priors.full <- c(set_prior("normal(0, .1)",
                           class = "Intercept"),
                 set_prior("normal(0, .5)",
                           class = "b"),
                 set_prior("normal(.5754, .1)",
                           # From Margoni & Surian (64%), logit(.64) = .5754 
                           class = "b", coef = "conditionsocial"),
                 set_prior("student_t(3, 0, 2)",
                           class = "sd"))
priors.noninformative <- set_prior("student_t(3, 0, 2)",
                                   class = "sd")

# Run model
brm.info.full <- brm(chose_helper ~ 1 + condition + z_age_days + condition:z_age_days +
                       (1 + condition + z_age_days + condition:z_age_days | lab_id), 
                     family = bernoulli, data = primary_data,
                     prior = priors.full, iter = 10000, 
                     control = list(adapt_delta = .99, max_treedepth = 20),
                     chains = 4, future = T, save_all_pars = TRUE)
brm.noninfo.full <- brm(chose_helper ~ 1 + condition + z_age_days + condition:z_age_days +
                       (1 + condition + z_age_days + condition:z_age_days | lab_id), 
                        family = bernoulli, data = primary_data,
                        prior = priors.noninformative, iter = 10000, 
                        control = list(adapt_delta = .99, max_treedepth = 20),
                        chains = 4, future = T, save_all_pars = TRUE)
```

We can already look at the parameter estimates and their 95% Credible Intervals from this model.

```{r full_estimates of each parameter, message=FALSE}
brm.info.full %>% estimates.brm_fixef(prob = .95) %>%
  kable()
brm.noninfo.full %>% estimates.brm_fixef(prob = .95) %>%
  kable()

```

Finally, we bridge-sample the posterior distribution of both the informative and non-informative model for later model comparison.

```{r full_bridge, message=FALSE}
bridge.info.full <- bridge_sampler(brm.info.full, silent = T)
bridge.noninfo.full <- bridge_sampler(brm.noninfo.full, silent = T)
```

### Choice preference

The first research question was whether or not infants in the social condition would chose the helper character more than infants in the non-social control condition, as evidenced by a greater-than-zero main effect of `condition`. To test this, we first define a null model, without the effect of interest. For the non-informative model, we use the same priors as for the full model.

```{r nocondition_model, results=FALSE, message=FALSE}
# Define priors
priors.no_condition <- c(set_prior("normal(0, .1)",
                           class = "Intercept"),
                 set_prior("normal(0, .5)",
                           class = "b"),
                 set_prior("student_t(3, 0, 2)",
                           class = "sd"))
# Run model
brm.info.no_condition <- brm(chose_helper ~ 1 + z_age_days +
                       (1 + z_age_days | lab_id), 
                     family = bernoulli, data = primary_data,
                     prior = priors.no_condition, iter = 10000, 
                     control = list(adapt_delta = .99, max_treedepth = 20),
                     chains = 4, future = T, save_all_pars = TRUE)
brm.noninfo.no_condition <- brm(chose_helper ~ 1 + z_age_days +
                          (1 + z_age_days | lab_id), 
                        family = bernoulli, data = primary_data,
                        prior = priors.noninformative, iter = 10000, 
                        control = list(adapt_delta = .99, max_treedepth = 20),
                        chains = 4, future = T, save_all_pars = TRUE)
```

We can now bridge-sample the posterior for this null model and compare it to the posterior from the full model to obtain a Bayes factor.

```{r bayes_nocondition message=FALSE}
# Bridge-sample posterior
bridge.info.no_condition <- bridge_sampler(brm.info.no_condition, silent = T)
bridge.noninfo.no_condition <- bridge_sampler(brm.noninfo.no_condition, silent = T)
# Compute Bayes factors
bf.info.condition <- bf(bridge.info.full, bridge.info.no_condition)
bf.noninfo.condition <- bf(bridge.noninfo.full, bridge.noninfo.no_condition)
print(bf.info.condition)
print(bf.noninfo.condition)
```


### Effect of age

The second research question we had was whether or not choice preference changed with age. To test this, we define a null model that does not have the interactive effect of `z_age_days:condition` and compare it to the full model.

```{r no_age, results=FALSE, message=FALSE}
# Define priors
priors.no_age <- c(set_prior("normal(0, .1)",
                           class = "Intercept"),
                 set_prior("normal(0, .5)",
                           class = "b"),
                 set_prior("normal(.5753641, .1)", # From Margoni & Surian, through logit
                           class = "b", coef = "conditionsocial"),
                 set_prior("student_t(3, 0, 2)",
                           class = "sd"))
# Run model
brm.info.no_age <- brm(chose_helper ~ 1 + condition + z_age_days +
                       (1 + condition + z_age_days | lab_id), 
                     family = bernoulli, data = primary_data,
                     prior = priors.no_age, iter = 10000, 
                     control = list(adapt_delta = .99, max_treedepth = 20),
                     chains = 4, future = T, save_all_pars = TRUE)
brm.noninfo.no_age <- brm(chose_helper ~ 1 + condition + z_age_days +
                          (1 + condition + z_age_days | lab_id), 
                        family = bernoulli, data = primary_data,
                        prior = priors.noninformative, iter = 10000, 
                        control = list(adapt_delta = .99, max_treedepth = 20),
                        chains = 4, future = T, save_all_pars = TRUE)
```


```{r no_age Bayes factor, message=FALSE}
# Bridge-sample posterior
bridge.info.no_age <- bridge_sampler(brm.info.no_age, silent = T)
bridge.noninfo.no_age <- bridge_sampler(brm.noninfo.no_age, silent = T)
# Compute Bayes factors
bf.info.age <- bf(bridge.info.full, bridge.info.no_age)
bf.noninfo.age <- bf(bridge.noninfo.full, bridge.noninfo.no_age)
print(bf.info.age)
print(bf.noninfo.age)
```


### Bayesian marginal effects

We plot the initial scatter plot by age with estimates from our full, non-informative Bayesian model.

```{r marginal_plot_noninfo, results=FALSE, message=FALSE}
# Compute marginal effects
marginal_effects <- brm.noninfo.full %>%
  ggpredict(terms = c("z_age_days [all]", "condition")) %>%
  rename(z_age_days = x,
         chose_helper = predicted,
         condition = group)

# Plot data and marginal effects
scatter.bayes <- ggplot(primary_data,
                  aes(x = z_age_days,
                      y = chose_helper,
                      colour = condition,
                      fill = condition)) +
  geom_line(data = marginal_effects) +
  geom_ribbon(alpha = .5, colour = NA,
              data = marginal_effects,
              aes(ymin = conf.low,
                  ymax = conf.high)) +
  geom_point(position = position_jitter(height = .05, width = 0),
             size = 1) +
  xlab("Age (scaled centred)") + ylab("Choice") + theme(legend.position = "top") +
  scale_colour_brewer(palette = "Dark2", name = "Condition",
                      breaks = c("nonsocial", "social"),
                      labels = c("non-social", "social")) +
  scale_fill_brewer(palette = "Dark2", name = "Condition",
                    breaks = c("nonsocial", "social"),
                    labels = c("non-social", "social")) +
  scale_y_continuous(breaks = c(0, 1),
                     labels = c("Hinderer/\nPush-down character", "Helper/\nPush-up character"))
ggsave("age_scatter_bayes_noninfo.pdf", scatter.bayes,
       units = "mm", width = 180, height = 100, dpi = 1000)
(scatter.bayes)


```

### Global Bayesian model (habituated infants)

As a follow-up analysis, we refitted the same model including only infants who successfully habituated to the events (n = 454 of babies successfully habituated).
```{r proportion of habituated infants}
# Calculate mean proportions for each condition
primary_data_habituated <- subset(primary_data, !is.na(habituation_trial))
nrow(primary_data_habituated)
nrow(primary_data_habituated)/nrow(primary_data)

proportions <- primary_data_habituated %>%
  group_by(condition) %>%
  summarize(
    mean_proportion = mean(chose_helper)
  )

# Print the results
print(paste0("Proportion of helper choice in the social condition: ", proportions$mean_proportion[proportions$condition == "social"]))
print(paste0("Proportion of helper choice in the nonsocial condition: ", proportions$mean_proportion[proportions$condition == "nonsocial"]))

```

```{r full_model_habituated, results=FALSE, message=FALSE}

# Define priors
priors.full <- c(set_prior("normal(0, .1)",
                           class = "Intercept"),
                 set_prior("normal(0, .5)",
                           class = "b"),
                 set_prior("normal(.5754, .1)",
                           # From Margoni & Surian (64%), logit(.64) = .5754 
                           class = "b", coef = "conditionsocial"),
                 set_prior("student_t(3, 0, 2)",
                           class = "sd"))
priors.noninformative <- set_prior("student_t(3, 0, 2)",
                                   class = "sd")

# Run model
brm.info.habituated.full <- brm(chose_helper ~ 1 + condition + z_age_days + condition:z_age_days +
                       (1 + condition + z_age_days + condition:z_age_days | lab_id), 
                     family = bernoulli, data = primary_data_habituated,
                     prior = priors.full, iter = 10000, 
                     control = list(adapt_delta = .99, max_treedepth = 20),
                     chains = 4, future = T, save_all_pars = TRUE)
brm.noninfo.habituated.full <- brm(chose_helper ~ 1 + condition + z_age_days + condition:z_age_days +
                       (1 + condition + z_age_days + condition:z_age_days | lab_id), 
                        family = bernoulli, data = primary_data_habituated,
                        prior = priors.noninformative, iter = 10000, 
                        control = list(adapt_delta = .99, max_treedepth = 20),
                        chains = 4, future = T, save_all_pars = TRUE)
```

We can already look at the parameter estimates and their 95% Credible Intervals from this model.

```{r full_estimates of each parameter_habituated, message=FALSE}
brm.info.habituated.full %>% estimates.brm_fixef(prob = .95) %>%
  kable()
brm.noninfo.habituated.full %>% estimates.brm_fixef(prob = .95) %>%
  kable()
```

Finally, we bridge-sample the posterior distribution of both the informative and non-informative model for later model comparison.

```{r full_bridge_habituated, message=FALSE}
bridge.info.habituated.full <- bridge_sampler(brm.info.habituated.full, silent = T)
bridge.noninfo.habituated.full <- bridge_sampler(brm.noninfo.habituated.full, silent = T)
```

### Choice preference (habituated infants)


```{r nocondition_model_habituated, results=FALSE, message=FALSE}
# Define priors
priors.no_condition <- c(set_prior("normal(0, .1)",
                           class = "Intercept"),
                 set_prior("normal(0, .5)",
                           class = "b"),
                 set_prior("student_t(3, 0, 2)",
                           class = "sd"))
# Run model
brm.info.habituated.no_condition <- brm(chose_helper ~ 1 + z_age_days +
                       (1 + z_age_days | lab_id), 
                     family = bernoulli, data = primary_data_habituated,
                     prior = priors.no_condition, iter = 10000, 
                     control = list(adapt_delta = .99, max_treedepth = 20),
                     chains = 4, future = T, save_all_pars = TRUE)
brm.noninfo.habituated.no_condition <- brm(chose_helper ~ 1 + z_age_days +
                          (1 + z_age_days | lab_id), 
                        family = bernoulli, data = primary_data_habituated,
                        prior = priors.noninformative, iter = 10000, 
                        control = list(adapt_delta = .99, max_treedepth = 20),
                        chains = 4, future = T, save_all_pars = TRUE)
```

We can now bridge-sample the posterior for this null model and compare it to the posterior from the full model to obtain a Bayes factor.

```{r bayes_nocondition_habituated message=FALSE}
# Bridge-sample posterior
bridge.info.habituated.no_condition <- bridge_sampler(brm.info.habituated.no_condition, silent = T)
bridge.noninfo.habituated.no_condition <- bridge_sampler(brm.noninfo.habituated.no_condition, silent = T)
# Compute Bayes factors
bf.info.habituated.condition <- bf(bridge.info.habituated.full, bridge.info.habituated.no_condition)
bf.noninfo.habituated.condition <- bf(bridge.noninfo.habituated.full, bridge.noninfo.habituated.no_condition)
print(bf.info.habituated.condition)
print(bf.noninfo.habituated.condition)
```

### Effect of age (habituated infants)

The second research question we had was whether or not choice preference changed with age. To test this, we define a null model that does not have the interactive effect of `z_age_days:condition` and compare it to the full model.

```{r no_age_habituated, results=FALSE, message=FALSE}
# Define priors
priors.no_age <- c(set_prior("normal(0, .1)",
                           class = "Intercept"),
                 set_prior("normal(0, .5)",
                           class = "b"),
                 set_prior("normal(.5753641, .1)", # From Margoni & Surian, through logit
                           class = "b", coef = "conditionsocial"),
                 set_prior("student_t(3, 0, 2)",
                           class = "sd"))
# Run model
brm.info.habituated.no_age <- brm(chose_helper ~ 1 + condition + z_age_days +
                       (1 + condition + z_age_days | lab_id), 
                     family = bernoulli, data = primary_data_habituated,
                     prior = priors.no_age, iter = 10000, 
                     control = list(adapt_delta = .99, max_treedepth = 20),
                     chains = 4, future = T, save_all_pars = TRUE)
brm.noninfo.habituated.no_age <- brm(chose_helper ~ 1 + condition + z_age_days +
                          (1 + condition + z_age_days | lab_id), 
                        family = bernoulli, data = primary_data_habituated,
                        prior = priors.noninformative, iter = 10000, 
                        control = list(adapt_delta = .99, max_treedepth = 20),
                        chains = 4, future = T, save_all_pars = TRUE)
```

```{r no_age Bayes factor_habituated, message=FALSE}
# Bridge-sample posterior
bridge.info.habituated.no_age <- bridge_sampler(brm.info.habituated.no_age, silent = T)
bridge.noninfo.habituated.no_age <- bridge_sampler(brm.noninfo.habituated.no_age, silent = T)
# Compute Bayes factors
bf.info.habituated.age <- bf(bridge.info.habituated.full, bridge.info.habituated.no_age)
bf.noninfo.habituated.age <- bf(bridge.noninfo.habituated.full, bridge.noninfo.habituated.no_age)
print(bf.info.habituated.age)
print(bf.noninfo.habituated.age)
```



## Lab variety analysis: ICCs

To look at the between-lab variability, we compute the intraclass-correlation for random intercepts of the mixed effects model.

```{r icc, message=FALSE}
primary_data_filtered <- primary_data[primary_data$lab_id != "corbitlab", ]

icc <- iccbin(cid = lab_id, y = chose_helper, 
              data = primary_data_filtered,
             alpha = 0.05)
print(icc)
```

### Choice preference (social condition)

Examine whether infants prefer helper over hinder in the social condition

```{r social_condition_intercept, results=FALSE, message=FALSE}
# select data
social_data <- subset(primary_data,condition == "social")
# Define priors
priors.single <- c(set_prior("normal(.5753641, .1)",
                           class = "Intercept"),
                 set_prior("normal(0, .5)",
                           class = "b"),
                 set_prior("student_t(3, 0, 2)",
                           class = "sd"))
priors.noninformative <- set_prior("student_t(3, 0, 2)",
                                   class = "sd")
# Run model
brm.info.single.full <- brm(chose_helper ~ 1 + z_age_days +
                       (1 + z_age_days | lab_id), 
                     family = bernoulli, data = social_data,
                     prior = priors.single, iter = 10000, 
                     control = list(adapt_delta = .99, max_treedepth = 20),
                     chains = 4, future = T, save_all_pars = TRUE)
brm.info.single.no_intercept <- brm(chose_helper ~ z_age_days +
                       (z_age_days | lab_id), 
                     family = bernoulli, data = social_data,
                     prior = priors.single, iter = 10000, 
                     control = list(adapt_delta = .99, max_treedepth = 20),
                     chains = 4, future = T, save_all_pars = TRUE)
brm.noninfo.single.full <- brm(chose_helper ~ 1 + z_age_days +
                       (1 + z_age_days | lab_id), 
                     family = bernoulli, data = social_data,
                     prior = priors.noninformative, iter = 10000, 
                     control = list(adapt_delta = .99, max_treedepth = 20),
                     chains = 4, future = T, save_all_pars = TRUE)
brm.noninfo.single.no_intercept <- brm(chose_helper ~ z_age_days +
                       (z_age_days | lab_id), 
                     family = bernoulli, data = social_data,
                     prior = priors.noninformative, iter = 10000, 
                     control = list(adapt_delta = .99, max_treedepth = 20),
                     chains = 4, future = T, save_all_pars = TRUE)
```

```{r social_condition_intercept Bayes factor, message=FALSE}
# Bridge-sample posterior
bridge.info.single.full <- bridge_sampler(brm.info.single.full, silent = T)
bridge.info.single.no_intercept <- bridge_sampler(brm.info.single.no_intercept, silent = T)
bridge.noninfo.single.full <- bridge_sampler(brm.noninfo.single.full, silent = T)
bridge.noninfo.single.no_intercept <- bridge_sampler(brm.noninfo.single.no_intercept, silent = T)

# Compute Bayes factors
bf.info.single.intercept <- bf(bridge.info.single.full, bridge.info.single.no_intercept)
bf.noninfo.single.intercept <- bf(bridge.noninfo.single.full, bridge.noninfo.single.no_intercept)
print(bf.info.single.intercept)
print(bf.noninfo.single.intercept)
```


```{r social_condition_intercept parameters, message=FALSE}
brm.info.single.full %>% estimates.brm_fixef(prob = .95) %>%
  kable()
brm.noninfo.single.full %>% estimates.brm_fixef(prob = .95) %>%
  kable()
```

# Exploraty analyses

## pre-registered exploratory analyses

We analyzed other potential moderators that were mentioned in our registered report, including (1) attention to the video events (i.e., as measured by the number of trials to habituation, overall looking time to the still frame events), (2) clear versus ambiguous choice actions (i.e., whether infants touched both characters during the choice phase), and (3) experimenter blindness (i.e., whether the experimenter administering the choice phase knew whether the infant participated in the Social vs. Non-Social condition). 

### total looking time

We analyzed the effect of overall looking time to the still frame events.

```{r total looking full_model, results=FALSE, message=FALSE}
# Define priors
priors.noninformative <- set_prior("student_t(3, 0, 2)",
                                   class = "sd")


brm.noninfo.total_looking <- brm(chose_helper ~ 1 + condition + total_looking + condition:total_looking +
                       (1 + condition + total_looking + condition:total_looking | lab_id), 
                        family = bernoulli, data = primary_data,
                        prior = priors.noninformative, iter = 10000, 
                        control = list(adapt_delta = .99, max_treedepth = 20),
                        chains = 4, future = T, save_all_pars = TRUE)
```

```{r total looking full_estimates of each parameter, message=FALSE}
brm.noninfo.total_looking %>% estimates.brm_fixef(prob = .95) %>%
  kable()

brm.noninfo.no_total_looking <- brm(chose_helper ~ 1 + condition + 
                       (1 + condition | lab_id), 
                        family = bernoulli, data = primary_data,
                        prior = priors.noninformative, iter = 10000, 
                        control = list(adapt_delta = .99, max_treedepth = 20),
                        chains = 4, future = T, save_all_pars = TRUE)
```

```{r total looking Bayes factor, results=FALSE, message=FALSE}
# Bridge-sample posterior
bridge.noninfo.total_looking <- bridge_sampler(brm.noninfo.total_looking, silent = T)
bridge.noninfo.no_total_looking <- bridge_sampler(brm.noninfo.no_total_looking, silent = T)

# Compute Bayes factors
bf.noninfo.total_looking <- bf(bridge.noninfo.total_looking, bridge.noninfo.no_total_looking)
print(bf.noninfo.total_looking)

```

We also separate looking time under helping/upward and hindering/downward scenarios and analyze their potential moderating effects.

```{r separated looking time full model, results=FALSE, message=FALSE}
# Define priors
priors.noninformative <- set_prior("student_t(3, 0, 2)",
                                   class = "sd")


brm.noninfo.looking <- brm(chose_helper ~ 1 + condition + up_looking +down_looking + condition:up_looking + condition:down_looking +
                       (1 + condition + + up_looking +down_looking + condition:up_looking + condition:down_looking| lab_id), 
                        family = bernoulli, data = primary_data,
                        prior = priors.noninformative, iter = 10000, 
                        control = list(adapt_delta = .99, max_treedepth = 20),
                        chains = 4, future = T, save_all_pars = TRUE)
```

```{r separated looking time full_estimates of each parameter, message=FALSE}
brm.noninfo.looking %>% estimates.brm_fixef(prob = .95) %>%
  kable()
```

### number of habituation trials

```{r habituation trials full_model, results=FALSE, message=FALSE}
# Define priors
priors.noninformative <- set_prior("student_t(3, 0, 2)",
                                   class = "sd")


brm.noninfo.hab_trial_num <- brm(chose_helper ~ 1 + condition + habituation_trial + condition:habituation_trial +
                       (1 + condition + habituation_trial + condition:habituation_trial | lab_id), 
                        family = bernoulli, data = primary_data,
                        prior = priors.noninformative, iter = 10000, 
                        control = list(adapt_delta = .99, max_treedepth = 20),
                        chains = 4, future = T, save_all_pars = TRUE)
```

```{r habituation trials full_estimates of each parameter, message=FALSE}
brm.noninfo.hab_trial_num %>% estimates.brm_fixef(prob = .95) %>%
  kable()
```

### ambiguous choice
```{r touch both full_model, results=FALSE, message=FALSE}
# Define priors
priors.noninformative <- set_prior("student_t(3, 0, 2)",
                                   class = "sd")


brm.noninfo.touch_both <- brm(chose_helper ~ 1 + condition + touch_both + condition:touch_both +
                       (1 + condition + touch_both + condition:touch_both | lab_id), 
                        family = bernoulli, data = primary_data,
                        prior = priors.noninformative, iter = 10000, 
                        control = list(adapt_delta = .99, max_treedepth = 20),
                        chains = 4, future = T, save_all_pars = TRUE)
```

```{r full_estimates of each parameter, message=FALSE}
brm.noninfo.touch_both %>% estimates.brm_fixef(prob = .95) %>%
  kable()
```

### naive experimenter

```{r naive RA full_model, results=FALSE, message=FALSE}
# Define priors
priors.noninformative <- set_prior("student_t(3, 0, 2)",
                                   class = "sd")


brm.noninfo.RA_1_naive <- brm(chose_helper ~ 1 + condition + RA_1_naive + condition:RA_1_naive +
                       (1 + condition + RA_1_naive + condition:RA_1_naive | lab_id), 
                        family = bernoulli, data = primary_data,
                        prior = priors.noninformative, iter = 10000, 
                        control = list(adapt_delta = .99, max_treedepth = 20),
                        chains = 4, future = T, save_all_pars = TRUE)
```

```{r full_estimates of each parameter, message=FALSE}
brm.noninfo.RA_1_naive %>% estimates.brm_fixef(prob = .95) %>%
  kable()
```

## non-pre-registered exploratory analyses

We also analyzed other potential moderators that were not mentioned in our registered report, including  (1) experiment-level factors (i.e., the order of helping vs. hindering videos, helper identity (yellow vs. blue), helper side during choice (right vs. left), the visual angle calculated by the distance of infants from screen and screen size, whether the hill paradigm was conducted in the first session during infant visits, whether the experimenter wore a mask or not, the presentation method of stimuli), (2) child-level characteristics (i.e., handedness, color blindness, participant gender, the exposure percentage of  primary language), and (3) lab-level factors (i.e., the exclusion rate of each lab, the rate of exclusion due to failure to make a choice for each lab).

### experimenter level
```{r push_up_order full_model, results=FALSE, message=FALSE}
# Define priors
priors.noninformative <- set_prior("student_t(3, 0, 2)",
                                   class = "sd")


brm.noninfo.push_up_order <- brm(chose_helper ~ 1 + condition + push_up_order + condition:push_up_order +
                       (1 + condition + push_up_order + condition:push_up_order | lab_id), 
                        family = bernoulli, data = primary_data,
                        prior = priors.noninformative, iter = 10000, 
                        control = list(adapt_delta = .99, max_treedepth = 20),
                        chains = 4, future = T, save_all_pars = TRUE)
```

```{r push_up_order full_estimates of each parameter, message=FALSE}
brm.noninfo.push_up_order %>% estimates.brm_fixef(prob = .95) %>%
  kable()
```

```{r push_up_identity full_model, results=FALSE, message=FALSE}
# Define priors
priors.noninformative <- set_prior("student_t(3, 0, 2)",
                                   class = "sd")


brm.noninfo.push_up_identity <- brm(chose_helper ~ 1 + condition + push_up_identity + condition:push_up_identity +
                       (1 + condition + push_up_identity + condition:push_up_identity | lab_id), 
                        family = bernoulli, data = primary_data,
                        prior = priors.noninformative, iter = 10000, 
                        control = list(adapt_delta = .99, max_treedepth = 20),
                        chains = 4, future = T, save_all_pars = TRUE)
```

```{r push_up_identity full_estimates of each parameter, message=FALSE}
brm.noninfo.push_up_identity %>% estimates.brm_fixef(prob = .95) %>%
  kable()
```

```{r push_up_side full_model, results=FALSE, message=FALSE}
# Define priors
priors.noninformative <- set_prior("student_t(3, 0, 2)",
                                   class = "sd")


brm.noninfo.push_up_side <- brm(chose_helper ~ 1 + condition + push_up_side + condition:push_up_side +
                       (1 + condition + push_up_side + condition:push_up_side | lab_id), 
                        family = bernoulli, data = primary_data,
                        prior = priors.noninformative, iter = 10000, 
                        control = list(adapt_delta = .99, max_treedepth = 20),
                        chains = 4, future = T, save_all_pars = TRUE)
```

```{r push_up_side full_estimates of each parameter, message=FALSE}
brm.noninfo.push_up_side %>% estimates.brm_fixef(prob = .95) %>%
  kable()
```

```{r visual_angle full_model, results=FALSE, message=FALSE}
primary_data$visual_angle <- 2 * atan(primary_data$screen_size_inches*2.54 / (2 * primary_data$infant_distance_cm)) * (180 / pi)

# Define priors
priors.noninformative <- set_prior("student_t(3, 0, 2)",
                                   class = "sd")


brm.noninfo.visual_angle <- brm(chose_helper ~ 1 + condition + visual_angle + condition:visual_angle +
                       (1 + condition + visual_angle + condition:visual_angle | lab_id), 
                        family = bernoulli, data = primary_data,
                        prior = priors.noninformative, iter = 10000, 
                        control = list(adapt_delta = .99, max_treedepth = 20),
                        chains = 4, future = T, save_all_pars = TRUE)
```

```{r visual_angle full_estimates of each parameter, message=FALSE}
brm.noninfo.visual_angle %>% estimates.brm_fixef(prob = .95) %>%
  kable()
```

```{r second_session full_model, results=FALSE, message=FALSE}
# Define priors
priors.noninformative <- set_prior("student_t(3, 0, 2)",
                                   class = "sd")


brm.noninfo.second_session <- brm(chose_helper ~ 1 + condition + second_session + condition:second_session +
                       (1 + condition + second_session + condition:second_session | lab_id), 
                        family = bernoulli, data = primary_data,
                        prior = priors.noninformative, iter = 10000, 
                        control = list(adapt_delta = .99, max_treedepth = 20),
                        chains = 4, future = T, save_all_pars = TRUE)
```

```{r second_session full_estimates of each parameter, message=FALSE}
brm.noninfo.second_session %>% estimates.brm_fixef(prob = .95) %>%
  kable()
```

```{r choice_experimenter_mask full_model, results=FALSE, message=FALSE}
# Define priors
priors.noninformative <- set_prior("student_t(3, 0, 2)",
                                   class = "sd")


brm.noninfo.choice_experimenter_mask <- brm(chose_helper ~ 1 + condition + choice_experimenter_mask + condition:choice_experimenter_mask +
                       (1 + condition + choice_experimenter_mask + condition:choice_experimenter_mask | lab_id), 
                        family = bernoulli, data = primary_data,
                        prior = priors.noninformative, iter = 10000, 
                        control = list(adapt_delta = .99, max_treedepth = 20),
                        chains = 4, future = T, save_all_pars = TRUE)
```

```{r choice_experimenter_mask full_estimates of each parameter, message=FALSE}
brm.noninfo.choice_experimenter_mask %>% estimates.brm_fixef(prob = .95) %>%
  kable()
```

```{r method full_model, results=FALSE, message=FALSE}
# Define priors
priors.noninformative <- set_prior("student_t(3, 0, 2)",
                                   class = "sd")


brm.noninfo.method <- brm(chose_helper ~ 1 + condition + method + condition:method +
                       (1 + condition + method + condition:method | lab_id), 
                        family = bernoulli, data = primary_data,
                        prior = priors.noninformative, iter = 10000, 
                        control = list(adapt_delta = .99, max_treedepth = 20),
                        chains = 4, future = T, save_all_pars = TRUE)
```

```{r method full_estimates of each parameter, message=FALSE}
brm.noninfo.method %>% estimates.brm_fixef(prob = .95) %>%
  kable()
```

### child level

```{r infant_handedness full_model, results=FALSE, message=FALSE}
# Define priors
priors.noninformative <- set_prior("student_t(3, 0, 2)",
                                   class = "sd")


brm.noninfo.infant_handedness <- brm(chose_helper ~ 1 + condition + infant_handedness + condition:infant_handedness +
                       (1 + condition + infant_handedness + condition:infant_handedness | lab_id), 
                        family = bernoulli, data = primary_data,
                        prior = priors.noninformative, iter = 10000, 
                        control = list(adapt_delta = .99, max_treedepth = 20),
                        chains = 4, future = T, save_all_pars = TRUE)
```

```{r infant_handedness full_estimates of each parameter, message=FALSE}
brm.noninfo.infant_handedness %>% estimates.brm_fixef(prob = .95) %>%
  kable()
```

```{r colorblindness_primaryfamily full_model, results=FALSE, message=FALSE}
# Define priors
priors.noninformative <- set_prior("student_t(3, 0, 2)",
                                   class = "sd")


brm.noninfo.colorblindness_primaryfamily <- brm(chose_helper ~ 1 + condition + colorblindness_primaryfamily + condition:colorblindness_primaryfamily +
                       (1 + condition + colorblindness_primaryfamily + condition:colorblindness_primaryfamily | lab_id), 
                        family = bernoulli, data = primary_data,
                        prior = priors.noninformative, iter = 10000, 
                        control = list(adapt_delta = .99, max_treedepth = 20),
                        chains = 4, future = T, save_all_pars = TRUE)
```

```{r colorblindness_primaryfamily full_estimates of each parameter, message=FALSE}
brm.noninfo.colorblindness_primaryfamily %>% estimates.brm_fixef(prob = .95) %>%
  kable()
```

```{r participant_gender full_model, results=FALSE, message=FALSE}
# Define priors
priors.noninformative <- set_prior("student_t(3, 0, 2)",
                                   class = "sd")


brm.noninfo.participant_gender <- brm(chose_helper ~ 1 + condition + participant_gender + condition:participant_gender +
                       (1 + condition + participant_gender + condition:participant_gender | lab_id), 
                        family = bernoulli, data = primary_data,
                        prior = priors.noninformative, iter = 10000, 
                        control = list(adapt_delta = .99, max_treedepth = 20),
                        chains = 4, future = T, save_all_pars = TRUE)
```

```{r participant_gender full_estimates of each parameter, message=FALSE}
brm.noninfo.participant_gender %>% estimates.brm_fixef(prob = .95) %>%
  kable()
```

```{r percent_primarylanguage full_model, results=FALSE, message=FALSE}
# need to exclude entry error for percentage of language (less than 20% should be excluded)
primary_data$percent_primarylanguage <- ifelse(primary_data$percent_primarylanguage<20,NA,primary_data$percent_primarylanguage)

# Define priors
priors.noninformative <- set_prior("student_t(3, 0, 2)",
                                   class = "sd")


brm.noninfo.percent_primarylanguage <- brm(chose_helper ~ 1 + condition + percent_primarylanguage + condition:percent_primarylanguage +
                       (1 + condition + percent_primarylanguage + condition:percent_primarylanguage | lab_id), 
                        family = bernoulli, data = primary_data,
                        prior = priors.noninformative, iter = 10000, 
                        control = list(adapt_delta = .99, max_treedepth = 20),
                        chains = 4, future = T, save_all_pars = TRUE)
```

```{r percent_primarylanguage full_estimates of each parameter, message=FALSE}
brm.noninfo.percent_primarylanguage %>% estimates.brm_fixef(prob = .95) %>%
  kable()
```

### lab level

```{r exclusion_rate full_model, results=FALSE, message=FALSE}
# Define priors
priors.noninformative <- set_prior("student_t(3, 0, 2)",
                                   class = "sd")


brm.noninfo.exclusion_rate <- brm(chose_helper ~ 1 + condition + exclusion_eligible + condition:exclusion_eligible +
                       (1 + condition + exclusion_eligible + condition:exclusion_eligible | lab_id), 
                        family = bernoulli, data = primary_data,
                        prior = priors.noninformative, iter = 10000, 
                        control = list(adapt_delta = .99, max_treedepth = 20),
                        chains = 4, future = T, save_all_pars = TRUE)
```

```{r exclusion_rate full_estimates of each parameter, message=FALSE}
brm.noninfo.exclusion_rate %>% estimates.brm_fixef(prob = .95) %>%
  kable()
```

```{r choice_exclusion_rate full_model, results=FALSE, message=FALSE}
# Define priors
priors.noninformative <- set_prior("student_t(3, 0, 2)",
                                   class = "sd")


brm.noninfo.choice_exclusion_rate <- brm(chose_helper ~ 1 + condition + choice_exclusion_rate + condition:choice_exclusion_rate +
                       (1 + condition + choice_exclusion_rate + condition:choice_exclusion_rate | lab_id), 
                        family = bernoulli, data = primary_data,
                        prior = priors.noninformative, iter = 10000, 
                        control = list(adapt_delta = .99, max_treedepth = 20),
                        chains = 4, future = T, save_all_pars = TRUE)
```

```{r choice_exclusion_rate full_estimates of each parameter, message=FALSE}
brm.noninfo.choice_exclusion_rate %>% estimates.brm_fixef(prob = .95) %>%
  kable()
```


## Exploratory analsyes separating habituated and nonhabituated infants 

We also analyzed whether infants were habituated or not would influence infants' choices under different conditions. We also exported the exact proportion of choosing the helper for infants who were habituated or not habibuated under two conditions.

### Plot (habituated vs. nonhabituated)

```{r habituation_plot}
primary_data$hab <- ifelse(is.na(primary_data$habituation_trial),0,1)

scatter <- ggplot(primary_data,
                  aes(x = hab,
                      y = chose_helper,
                      colour = condition)) +
  stat_smooth(method = "lm") +
  geom_point(position = position_jitter(height = .05, width = 0),
             size = 1) +
  xlab("Habituation or not") + ylab("Choice") + theme(legend.position = "top") +
  scale_colour_brewer(palette = "Dark2", name = "Condition",
                      breaks = c("nonsocial", "social"),
                      labels = c("non-social", "social")) +
  scale_y_continuous(breaks = c(0, 1),
                     labels = c("Hinderer/\nPush-down character", "Helper/\nPush-up character"))+
  scale_x_continuous(breaks = c(0, 1),
                     labels = c("unhabituated", "habituated"))
ggsave("hab_scatter.pdf", scatter,
       units = "mm", width = 180, height = 100, dpi = 1000)
(scatter)

# Create the interaction variable
primary_data$Interaction <- interaction(primary_data$condition, primary_data$hab)

result <- tapply(primary_data$chose_helper, primary_data$Interaction, FUN = function(x) mean(x))  # You can use any summary function here

print(result)
```

### Bayesian model adding habituation as a moderator

Whether infants were habituated or not was added to the full model.

```{r habituation or not full_model, results=FALSE, message=FALSE}
# Define priors
priors.noninformative <- set_prior("student_t(3, 0, 2)",
                                   class = "sd")

# Run model
brm.noninfo.full <- brm(chose_helper ~ 1 + condition + hab + condition:hab + 
                       (1 + condition + hab + condition:hab | lab_id),
                        family = bernoulli, data = primary_data,
                        prior = priors.noninformative, iter = 10000, 
                        control = list(adapt_delta = .99, max_treedepth = 20),
                        chains = 4, future = T, save_all_pars = TRUE)
```

We can already look at the parameter estimates and their 95% Credible Intervals from this model.

```{r habituation or not full_estimates, message=FALSE}
brm.noninfo.full %>% estimates.brm_fixef(prob = .95) %>%
  kable()
```

Finally, we bridge-sample the posterior distribution of both the informative and non-informative model for later model comparison.

```{r habituation or not full_bridge}
bridge.noninfo.full <- bridge_sampler(brm.noninfo.full, silent = T)
```

We calculated the BF for the interaction between habituation and condition

```{r nohabcondition_model, results=FALSE, message=FALSE}

# Run model
brm.noninfo.no_hab <- brm(chose_helper ~ 1 + condition + hab + 
                       (1 + condition + hab  | lab_id), 
                        family = bernoulli, data = primary_data,
                        prior = priors.noninformative, iter = 10000, 
                        control = list(adapt_delta = .99, max_treedepth = 20),
                        chains = 4, future = T, save_all_pars = TRUE)
```


```{r bayes_nohabcondition}
# Bridge-sample posterior
bridge.noninfo.no_hab <- bridge_sampler(brm.noninfo.no_hab, silent = T)
# Compute Bayes factors
bf.noninfo.hab <- bf(bridge.noninfo.full, bridge.noninfo.no_hab)
print(bf.noninfo.hab)
```

### Choice preference when habituated

We examine whether infants preferred helper more in the social condition than in the non-social condition when they were habituated. 

```{r habituated condition models, results=FALSE, message=FALSE}
# select data
hab_data <- subset(primary_data,hab == 1)
# Define priors
priors.noninformative <- set_prior("student_t(3, 0, 2)",
                                   class = "sd")
# Run model
brm.noninfo.hab.full <- brm(chose_helper ~ 1 + condition + 
                       (1 + condition | lab_id), 
                     family = bernoulli, data = hab_data,
                     prior = priors.noninformative, iter = 10000, 
                     control = list(adapt_delta = .99, max_treedepth = 20),
                     chains = 4, future = T, save_all_pars = TRUE)
brm.noninfo.hab.no_condition <- brm(chose_helper ~ 1 + 
                       (1| lab_id), 
                     family = bernoulli, data = hab_data,
                     prior = priors.noninformative, iter = 10000, 
                     control = list(adapt_delta = .99, max_treedepth = 20),
                     chains = 4, future = T, save_all_pars = TRUE)

```

```{r habituated condition Bayes factor, results=FALSE, message=FALSE}
# Bridge-sample posterior
bridge.noninfo.hab.full <- bridge_sampler(brm.noninfo.hab.full, silent = T)
bridge.noninfo.hab.no_condition <- bridge_sampler(brm.noninfo.hab.no_condition, silent = T)

# Compute Bayes factors
bf.noninfo.hab.condition <- bf(bridge.noninfo.hab.full, bridge.noninfo.hab.no_condition)
print(bf.noninfo.hab.condition)

```

```{r habituated condition parameters, results=FALSE, message=FALSE}
brm.noninfo.hab.full %>% estimates.brm_fixef(prob = .95) %>%
  kable()
```

### Choice preference when not habituated

We examine whether infants prefer helper more in the social condition than in the non-social condition when they were not habituated 

```{r nonhabituated condition models, results=FALSE, message=FALSE}
# select data
nonhab_data <- subset(primary_data,hab == 0)
# Define priors
priors.noninformative <- set_prior("student_t(3, 0, 2)",
                                   class = "sd")
# Run model
brm.noninfo.nonhab.full <- brm(chose_helper ~ 1 + condition + 
                       (1 + condition | lab_id), 
                     family = bernoulli, data = nonhab_data,
                     prior = priors.noninformative, iter = 10000, 
                     control = list(adapt_delta = .99, max_treedepth = 20),
                     chains = 4, future = T, save_all_pars = TRUE)
brm.noninfo.nonhab.no_condition <- brm(chose_helper ~ 1 + 
                       (1| lab_id), 
                     family = bernoulli, data = nonhab_data,
                     prior = priors.noninformative, iter = 10000, 
                     control = list(adapt_delta = .99, max_treedepth = 20),
                     chains = 4, future = T, save_all_pars = TRUE)

```

```{r nonhabituated condition Bayes factor, results=FALSE, message=FALSE}
# Bridge-sample posterior
bridge.noninfo.nonhab.full <- bridge_sampler(brm.noninfo.nonhab.full, silent = T)
bridge.noninfo.nonhab.no_condition <- bridge_sampler(brm.noninfo.nonhab.no_condition, silent = T)

# Compute Bayes factors
bf.noninfo.nonhab.condition <- bf(bridge.noninfo.nonhab.full, bridge.noninfo.nonhab.no_condition)
print(bf.noninfo.nonhab.condition)
```

```{r nonhabituated condition parameters, message=FALSE}
brm.noninfo.nonhab.full %>% estimates.brm_fixef(prob = .95) %>%
  kable()
```
### Choice preference in the social condition

We examine whether being habituated or not has an effect on infants choices in the social condition. 

```{r social condition models, results=FALSE, message=FALSE}
# select data
social_data <- subset(primary_data,condition == "social")

# Define priors
priors.noninformative <- set_prior("student_t(3, 0, 2)",
                                   class = "sd")
# Run model
brm.noninfo.social.full <- brm(chose_helper ~ 1 + hab + 
                       (1 + hab | lab_id), 
                     family = bernoulli, data = social_data,
                     prior = priors.noninformative, iter = 10000, 
                     control = list(adapt_delta = .99, max_treedepth = 20),
                     chains = 4, future = T, save_all_pars = TRUE)
brm.noninfo.social.no_hab <- brm(chose_helper ~ 1 + 
                       (1 | lab_id), 
                     family = bernoulli, data = social_data,
                     prior = priors.noninformative, iter = 10000, 
                     control = list(adapt_delta = .99, max_treedepth = 20),
                     chains = 4, future = T, save_all_pars = TRUE)

```

```{r social condition Bayes factor, results=FALSE, message=FALSE}
# Bridge-sample posterior
bridge.noninfo.social.full <- bridge_sampler(brm.noninfo.social.full, silent = T)
bridge.noninfo.social.no_hab <- bridge_sampler(brm.noninfo.social.no_hab, silent = T)

# Compute Bayes factors
bf.noninfo.social.hab <- bf(bridge.noninfo.social.full, bridge.noninfo.social.no_hab)
print(bf.noninfo.social.hab)

```

```{r social condition parameters, results=FALSE, message=FALSE}
brm.noninfo.social.full %>% estimates.brm_fixef(prob = .95) %>%
  kable()
```

### Choice preference in the nonsocial condition

We examine whether being habituated or not has an effect on infants choices in the nonsocial condition. 

```{r nonsocial condition models, results=FALSE, message=FALSE}
# select data
nonsocial_data <- subset(primary_data,condition == "nonsocial")

# Define priors
priors.noninformative <- set_prior("student_t(3, 0, 2)",
                                   class = "sd")
# Run model
brm.noninfo.nonsocial.full <- brm(chose_helper ~ 1 + hab+
                       (1 + hab | lab_id), 
                     family = bernoulli, data = nonsocial_data,
                     prior = priors.noninformative, iter = 10000, 
                     control = list(adapt_delta = .99, max_treedepth = 20),
                     chains = 4, future = T, save_all_pars = TRUE)
brm.noninfo.nonsocial.no_hab <- brm(chose_helper ~ 1 + 
                       (1 | lab_id), 
                     family = bernoulli, data = nonsocial_data,
                     prior = priors.noninformative, iter = 10000, 
                     control = list(adapt_delta = .99, max_treedepth = 20),
                     chains = 4, future = T, save_all_pars = TRUE)

```

```{r nonsocial condition Bayes factor, results=FALSE, message=FALSE}
# Bridge-sample posterior
bridge.noninfo.nonsocial.full <- bridge_sampler(brm.noninfo.nonsocial.full, silent = T)
bridge.noninfo.nonsocial.no_hab <- bridge_sampler(brm.noninfo.nonsocial.no_hab, silent = T)

# Compute Bayes factors
bf.noninfo.nonsocial.hab <- bf(bridge.noninfo.nonsocial.full, bridge.noninfo.nonsocial.no_hab)
print(bf.noninfo.nonsocial.hab)

```

```{r nonsocial condition parameters, results=FALSE, message=FALSE}
brm.noninfo.nonsocial.full %>% estimates.brm_fixef(prob = .95) %>%
  kable()
```

## exploratory analyses of looking time

### total looking time

To test for the possibility of attentional differences across the Social and Non-Social conditions, we conducted exploratory analyses on infants gaze behavior during habituation, and in particular how long they look at the still image of the final frame presented after each video.

We first analyzed the effect of condition.

```{r full_exploratory_model total looking and condition, results=FALSE, message=FALSE}
# Define priors
priors.noninformative <- set_prior("student_t(3, 0, 2)",
                                   class = "sd")

# Run model
brm.noninfo.looking.full <- brm(total_looking ~ 1 + condition + z_age_days + condition:z_age_days + (1 + condition + z_age_days + condition:z_age_days|lab_id), 
                         data = primary_data,
                        prior = priors.noninformative, iter = 10000, 
                        control = list(adapt_delta = .99, max_treedepth = 20),
                        chains = 4, future = T, save_all_pars = TRUE)
brm.noninfo.looking.no_condition <- brm(total_looking ~ 1 + z_age_days + 
                                          (1 + z_age_days|lab_id), 
                        data = primary_data,
                        prior = priors.noninformative, iter = 10000, 
                        control = list(adapt_delta = .99, max_treedepth = 20),
                        chains = 4, future = T, save_all_pars = TRUE)
```

```{r Bayes factor total looking and condition, results=FALSE, message=FALSE}
# Bridge-sample posterior
bridge.noninfo.looking.full <- bridge_sampler(brm.noninfo.looking.full, silent = T)
bridge.noninfo.looking.no_condition <- bridge_sampler(brm.noninfo.looking.no_condition, silent = T)

# Compute Bayes factors
bf.noninfo.looking.condition <- bf(bridge.noninfo.looking.full, bridge.noninfo.looking.no_condition)
print(bf.noninfo.looking.condition)
```
```{r description total looking and condition, results=FALSE, message=FALSE}
# Calculate mean and standard deviation for each condition
mean_socialcondition <- mean(primary_data$total_looking[primary_data$condition == "social"])
sd_socialcondition <- sd(primary_data$total_looking[primary_data$condition == "social"])

mean_nonsocialcondition <- mean(primary_data$total_looking[primary_data$condition == "nonsocial"])
sd_nonsocialcondition <- sd(primary_data$total_looking[primary_data$condition == "nonsocial"])

# Print results
cat("Mean looking time for Social Condition:", mean_socialcondition, "\n")
cat("Standard deviation of looking time for Social Condition:", sd_socialcondition, "\n")

cat("Mean looking time for Nonsocial Condition:", mean_nonsocialcondition, "\n")
cat("Standard deviation of looking time for Nonsocial Condition:", sd_nonsocialcondition, "\n")
```

We then analyzed the effect of age by removing the interaction between age and condition from the full model.

```{r full.exploratory_model total looking and age, results=FALSE, message=FALSE}
# Define priors
priors.noninformative <- set_prior("student_t(3, 0, 2)",
                                   class = "sd")

# Run model
brm.noninfo.looking.no_age <- brm(total_looking ~ 1 + z_age_days + condition  + (1 + z_age_days + condition|lab_id), 
                        data = primary_data,
                        prior = priors.noninformative, iter = 10000, 
                        control = list(adapt_delta = .99, max_treedepth = 20),
                        chains = 4, future = T, save_all_pars = TRUE)
```


```{r Bayes factor total looking time and age, results=FALSE, message=FALSE}
# Bridge-sample posterior
bridge.noninfo.looking.full <- bridge_sampler(brm.noninfo.looking.full, silent = T)
bridge.noninfo.looking.no_age <- bridge_sampler(brm.noninfo.looking.no_age, silent = T)

# Compute Bayes factors
bf.noninfo.looking.age <- bf(bridge.noninfo.looking.full, bridge.noninfo.looking.no_age)
print(bf.noninfo.looking.age)

```

```{r looking time parameters, results=FALSE, message=FALSE}
brm.noninfo.looking.full %>% estimates.brm_fixef(prob = .95) %>%
  kable()
```

```{r full.exploratory_model total looking and age (young), results=FALSE, message=FALSE}
# Define priors
priors.noninformative <- set_prior("student_t(3, 0, 2)",
                                   class = "sd")
young_data <- subset(primary_data,z_age_days < -1)

# Run model
brm.noninfo.looking.full.young <- brm(total_looking ~ 1 + condition + (1 + condition|lab_id), 
                        data = young_data,
                        prior = priors.noninformative, iter = 10000, 
                        control = list(adapt_delta = .99, max_treedepth = 20),
                        chains = 4, future = T, save_all_pars = TRUE)
brm.noninfo.looking.no_condition.young <- brm(total_looking ~ 1 +  (1 |lab_id), 
                        data = young_data,
                        prior = priors.noninformative, iter = 10000, 
                        control = list(adapt_delta = .99, max_treedepth = 20),
                        chains = 4, future = T, save_all_pars = TRUE)
```
```{r looking time parameters (young), results=FALSE, message=FALSE}
brm.noninfo.looking.full.young %>% estimates.brm_fixef(prob = .95) %>%
  kable()

# Calculate mean and standard deviation for each condition
mean_socialcondition <- mean(young_data$total_looking[young_data$condition == "social"])
sd_socialcondition <- sd(young_data$total_looking[young_data$condition == "social"])

mean_nonsocialcondition <- mean(young_data$total_looking[young_data$condition == "nonsocial"])
sd_nonsocialcondition <- sd(young_data$total_looking[young_data$condition == "nonsocial"])

# Print results
cat("Mean looking time for Social Condition:", mean_socialcondition, "\n")
cat("Standard deviation of looking time for Social Condition:", sd_socialcondition, "\n")

cat("Mean looking time for Nonsocial Condition:", mean_nonsocialcondition, "\n")
cat("Standard deviation of looking time for Nonsocial Condition:", sd_nonsocialcondition, "\n")


```
```{r Bayes factor total looking time and age (young), results=FALSE, message=FALSE}
# Bridge-sample posterior
bridge.noninfo.looking.full.young <- bridge_sampler(brm.noninfo.looking.full.young, silent = T)
bridge.noninfo.looking.no_condition.young <- bridge_sampler(brm.noninfo.looking.no_condition.young, silent = T)

# Compute Bayes factors
bf.noninfo.looking.age.young <- bf(bridge.noninfo.looking.full.young, bridge.noninfo.looking.no_condition.young)
print(bf.noninfo.looking.age.young)

```

```{r full.exploratory_model total looking and age (old), results=FALSE, message=FALSE}
# Define priors
priors.noninformative <- set_prior("student_t(3, 0, 2)",
                                   class = "sd")
old_data <- subset(primary_data,z_age_days > 1)

# Run model
brm.noninfo.looking.full.old <- brm(total_looking ~ 1 + condition + (1 + condition|lab_id), 
                        data = old_data,
                        prior = priors.noninformative, iter = 10000, 
                        control = list(adapt_delta = .99, max_treedepth = 20),
                        chains = 4, future = T, save_all_pars = TRUE)
brm.noninfo.looking.no_condition.old <- brm(total_looking ~ 1 +  (1 |lab_id), 
                        data = old_data,
                        prior = priors.noninformative, iter = 10000, 
                        control = list(adapt_delta = .99, max_treedepth = 20),
                        chains = 4, future = T, save_all_pars = TRUE)
```
```{r looking time parameters (old), results=FALSE, message=FALSE}
brm.noninfo.looking.full.old %>% estimates.brm_fixef(prob = .95) %>%
  kable()
```
```{r Bayes factor total looking time and age (old), results=FALSE, message=FALSE}
# Bridge-sample posterior
bridge.noninfo.looking.full.old <- bridge_sampler(brm.noninfo.looking.full.old, silent = T)
bridge.noninfo.looking.no_condition.old <- bridge_sampler(brm.noninfo.looking.no_condition.old, silent = T)

# Compute Bayes factors
bf.noninfo.looking.age.old <- bf(bridge.noninfo.looking.full.old, bridge.noninfo.looking.no_condition.old)
print(bf.noninfo.looking.age.old)

```


```{r full.exploratory_model total looking and age (social), results=FALSE, message=FALSE}
# Define priors
priors.noninformative <- set_prior("student_t(3, 0, 2)",
                                   class = "sd")

# Run model
brm.noninfo.looking.full.social <- brm(total_looking ~ 1 + z_age_days + (1 + z_age_days|lab_id), 
                        data = social_data,
                        prior = priors.noninformative, iter = 10000, 
                        control = list(adapt_delta = .99, max_treedepth = 20),
                        chains = 4, future = T, save_all_pars = TRUE)
brm.noninfo.looking.no_age.social <- brm(total_looking ~ 1 +  (1 |lab_id), 
                        data = social_data,
                        prior = priors.noninformative, iter = 10000, 
                        control = list(adapt_delta = .99, max_treedepth = 20),
                        chains = 4, future = T, save_all_pars = TRUE)
```
```{r looking time parameters (social), results=FALSE, message=FALSE}
brm.noninfo.looking.full.social %>% estimates.brm_fixef(prob = .95) %>%
  kable()
```

```{r Bayes factor total looking time and age (social), results=FALSE, message=FALSE}
# Bridge-sample posterior
bridge.noninfo.looking.full.social <- bridge_sampler(brm.noninfo.looking.full.social, silent = T)
bridge.noninfo.looking.no_age.social <- bridge_sampler(brm.noninfo.looking.no_age.social, silent = T)

# Compute Bayes factors
bf.noninfo.looking.age.social <- bf(bridge.noninfo.looking.full.social, bridge.noninfo.looking.no_age.social)
print(bf.noninfo.looking.age.social)

```


```{r full.exploratory_model total looking and age (nonsocial), results=FALSE, message=FALSE}
# Define priors
priors.noninformative <- set_prior("student_t(3, 0, 2)",
                                   class = "sd")

# Run model
brm.noninfo.looking.full.nonsocial <- brm(total_looking ~ 1 + z_age_days + (1 + z_age_days|lab_id), 
                        data = nonsocial_data,
                        prior = priors.noninformative, iter = 10000, 
                        control = list(adapt_delta = .99, max_treedepth = 20),
                        chains = 4, future = T, save_all_pars = TRUE)
brm.noninfo.looking.no_age.nonsocial <- brm(total_looking ~ 1 +  (1 |lab_id), 
                        data = nonsocial_data,
                        prior = priors.noninformative, iter = 10000, 
                        control = list(adapt_delta = .99, max_treedepth = 20),
                        chains = 4, future = T, save_all_pars = TRUE)
```
```{r looking time parameters (nonsocial), results=FALSE, message=FALSE}
brm.noninfo.looking.full.nonsocial %>% estimates.brm_fixef(prob = .95) %>%
  kable()
```

```{r Bayes factor total looking time and age (nonsocial), results=FALSE, message=FALSE}
# Bridge-sample posterior
bridge.noninfo.looking.full.nonsocial <- bridge_sampler(brm.noninfo.looking.full.nonsocial, silent = T)
bridge.noninfo.looking.no_age.nonsocial <- bridge_sampler(brm.noninfo.looking.no_age.nonsocial, silent = T)

# Compute Bayes factors
bf.noninfo.looking.age.nonsocial <- bf(bridge.noninfo.looking.full.nonsocial, bridge.noninfo.looking.no_age.nonsocial)
print(bf.noninfo.looking.age.nonsocial)

```


```{r total looking time age_plot, message=FALSE}
scatter <- ggplot(primary_data,
                  aes(x = z_age_days,
                      y = total_looking,
                      colour = condition)) +
  stat_smooth(method = "lm") +
  geom_point(position = position_jitter(height = .05, width = 0),
             size = 1) +
  xlab("Age (scaled centred)") + ylab("looking time") + theme(legend.position = "top") +
  scale_colour_brewer(palette = "Dark2", name = "Condition",
                      breaks = c("nonsocial", "social"),
                      labels = c("non-social", "social")) 
ggsave("age_scatter_looking.pdf", scatter,
       units = "mm", width = 180, height = 100, dpi = 1000)
(scatter)
```


```{r looking time marginal_plot, results=FALSE, message=FALSE}
# Compute marginal effects
marginal_effects <- brm.noninfo.looking.full %>%
  ggpredict(terms = c("z_age_days [all]", "condition")) %>%
  rename(z_age_days = x,
         total_looking = predicted,
         condition = group)

primary_data$condition = factor(primary_data$condition, c("nonsocial","social"))
marginal_effects$condition = factor(marginal_effects$condition, c("nonsocial","social"))

# Plot data and marginal effects
scatter.bayes <- ggplot(primary_data,
                  aes(x = z_age_days,
                      y = total_looking,
                      colour = condition,
                      fill = condition)) +
  geom_line(data = marginal_effects) +
  geom_ribbon(alpha = .5, colour = NA,
              data = marginal_effects,
              aes(ymin = conf.low,
                  ymax = conf.high)) +
  geom_point(position = position_jitter(height = .05, width = 0),
             size = 1) +
  xlab("Age (scaled centred)") + ylab("Looking time") + theme(legend.position = "top") +
  scale_colour_brewer(palette = "Dark2", name = "Condition",
                      breaks = c("nonsocial","social"),
                      labels = c("non-social","social")) +
  scale_fill_brewer(palette = "Dark2", name = "Condition",
                   breaks = c("nonsocial","social"),
                      labels = c("non-social","social")) 
ggsave("age_scatter_bayes_looking.pdf", scatter.bayes,
       units = "mm", width = 180, height = 100, dpi = 1000)
(scatter.bayes)

```


### looking time in helping/upward condition and in hindering/downward condition

Additional analyses separated infant looking time in helping/upward condition and in hindering/downward condition.Infants looked longer at the still frame in the helping social condition than in the upward non-social condition and looked longer at the still frame in the hindering social condition than in the downward non-social condition. 

```{r up_looking_condition full_model, results=FALSE, message=FALSE}
# Define priors
priors.noninformative <- set_prior("student_t(3, 0, 2)",
                                   class = "sd")


brm.noninfo.up_looking_condition <- brm(up_looking ~ 1 + condition + z_age_days + condition:z_age_days +
                       (1 + condition + z_age_days + condition:z_age_days | lab_id), 
                        data = primary_data,
                        prior = priors.noninformative, iter = 10000, 
                        control = list(adapt_delta = .99, max_treedepth = 20),
                        chains = 4, future = T, save_all_pars = TRUE)
```

```{r up_looking full_estimates of each parameter, message=FALSE}
brm.noninfo.up_looking_condition %>% estimates.brm_fixef(prob = .95) %>%
  kable()
```

```{r down_looking_condition full_model, results=FALSE, message=FALSE}
# Define priors
priors.noninformative <- set_prior("student_t(3, 0, 2)",
                                   class = "sd")


brm.noninfo.down_looking_condition <- brm(down_looking ~ 1 + condition + z_age_days + condition:z_age_days +
                       (1 + condition + z_age_days + condition:z_age_days | lab_id), 
                        data = primary_data,
                        prior = priors.noninformative, iter = 10000, 
                        control = list(adapt_delta = .99, max_treedepth = 20),
                        chains = 4, future = T, save_all_pars = TRUE)
```

```{r down_looking full_estimates of each parameter, message=FALSE}
brm.noninfo.down_looking_condition %>% estimates.brm_fixef(prob = .95) %>%
  kable()
```

### compare looking time toward helping vs. hindering scenario

We also compared infants' looking time in helping vs. hindering condition. Infants spent a comparable amount of time looking at the still frame following videos depicting helping/upward movements and videos depicting hindering/downward movements.

```{r up_looking vs. down_looking, message=FALSE}
library(tidyr)
short_data <- primary_data[, c("subj_num","up_looking", "down_looking","condition","lab_id")]

# Convert short data to long data
primary_looking_data <- gather(short_data, key = "identity", value = "time", -subj_num,-lab_id,-condition)

```

```{r up_looking vs. down_looking full_model, results=FALSE, message=FALSE}
# Define priors
priors.noninformative <- set_prior("student_t(3, 0, 2)",
                                   class = "sd")


brm.noninfo.up_down <- brm(time ~ 1 + condition + identity + condition:identity +
                       (1 + condition + identity + condition:identity | lab_id), 
                        data = primary_looking_data,
                        prior = priors.noninformative, iter = 10000, 
                        control = list(adapt_delta = .99, max_treedepth = 20),
                        chains = 4, future = T, save_all_pars = TRUE)
```

```{r up_looking vs. down_looking full_estimates of each parameter, message=FALSE}
brm.noninfo.up_down %>% estimates.brm_fixef(prob = .95) %>%
  kable()
```
